[
  {
    "objectID": "academia/index.html",
    "href": "academia/index.html",
    "title": "Academic Activities",
    "section": "",
    "text": "2023 - The 10th Statistical Analysis of Neuronal Data (SAND10) Workshop - Talk\n- Towards a simplified model of primary visual cortex\n2021 - Computational and Systems Neuroscience (COSYNE) - Poster\n- Invariant texture recognition in mouse visual cortex, Fengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer"
  },
  {
    "objectID": "academia/index.html#posters-and-talks",
    "href": "academia/index.html#posters-and-talks",
    "title": "Academic Activities",
    "section": "",
    "text": "2023 - The 10th Statistical Analysis of Neuronal Data (SAND10) Workshop - Talk\n- Towards a simplified model of primary visual cortex\n2021 - Computational and Systems Neuroscience (COSYNE) - Poster\n- Invariant texture recognition in mouse visual cortex, Fengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer"
  },
  {
    "objectID": "academia/index.html#teaching-activities",
    "href": "academia/index.html#teaching-activities",
    "title": "Academic Activities",
    "section": "Teaching Activities",
    "text": "Teaching Activities\n2025 ‚Äî Cajal NeuroAI Summer School ‚Äî Teaching Assistant\n- Designed and delivered tutorials on computational neuroscience and deep learning.\n- Mentored students on projects involving foundation models and unsupervised learning using JEPA.\n2023 ‚Äî Probabilistic Machine Learning ‚Äî Teaching Assistant\n- Organized and assisted in teaching the course for neuroscience students at Janelia.\n- Based on the Probabilistic Machine Learning book.\n2021 ‚Äî Neuromatch Academy ‚Äî Teaching Assistant\n- course TA of Neuromatch computational neuroscience course.\n2020 ‚Äî Neuromatch Academy ‚Äî Teaching Assistant\n- course TA of Neuromatch computational neuroscience course."
  },
  {
    "objectID": "journals/index.html",
    "href": "journals/index.html",
    "title": "Journal",
    "section": "",
    "text": "My First Post\n\n\n\n\n\n\n\n\nAug 17, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Du Fengtong (Farah)",
    "section": "",
    "text": "Du Fengtong\n\n\n GitHub\n Twitter\n Google Scholar\n\n\nHi! My name is Fengtong (Farah) Du, a PhD student in the Janelia-JHU Joint Neuroscience Graduate Program, advised by Carsen Stringer.\nMy work bridges computational modeling and neuroscience, with a focus on understanding how visual information is represented in the brain, and how the brain learn from the outside world in an unsupervised way.\nOutside the lab, I enjoy Chinese philosophy, Russian literature, writing and music. I also love occasional spontaneous adventure close to home.\nWelcome to check out my CV"
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Du Fengtong (Farah)",
    "section": "Selected Projects",
    "text": "Selected Projects"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Towards a simplified model of primary visual cortex\n\n\nSimplified and interpretable ‚Äúminimodels‚Äù are sufficient to explain complex visual responses in mouse and monkey V1.\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAudio‚ÄìVisual Integration Model (AVIM) for Continual Learning\n\n\nAVIM is a multimodal spiking model that fuses visual and audio inputs and learns with a Synaptic Tagging & Capture‚Äìlike rule. It targets brain-inspired continual learning with reduced catastrophic forgetting.\n\n\n\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nGastroenterologist-level detection of gastric precursor lesion\n\n\nDigestive Endoscopology Recognition.\n\n\n\n\n\nJul 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSports Object Recognition\n\n\nObject detection from basketball and football videos.\n\n\n\n\n\nAug 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBullet hole detection\n\n\nBullet hole detection using series Faster-RCNN and video analysis.\n\n\n\n\n\nJul 1, 2017\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/posts/AVIM.html",
    "href": "projects/posts/AVIM.html",
    "title": "Continual learning through multisensory integration in spiking neural network",
    "section": "",
    "text": "Abstract\n\n\n\nHuman continual learning relies on multisensory integration across medial temporal lobe (MTL) circuits. Inspired by this, we build an Audio-Visual Integration Model (AVIM) in a spiking neural network that fuses a visual feature vector (from a lightweight CNN) with an audio representation produced by a randomized near-orthogonal sparse coder (NOSC). The network uses Synaptic Tagging and Capture (STC)-like plasticity (via an NIMI-STC module) to maintain long-term memories while acquiring new classes. With a LOC-ANN readout, AVIM achieves robust performance in class-incremental settings on MNIST, EMNIST, CIFAR-10, and CIFAR-100, showing stronger retention of old knowledge compared to baselines without STC or without multimodal fusion.\n\n\nposter (PDF) | code (coming soon)\n\n\nProject highlights\n\nNeuro-inspired continual learning ‚Äî Motivated by MTL‚ÄìPER‚ÄìEC pathways and multisensory integration to reduce catastrophic forgetting.\n\nAudio‚Äìvisual fusion in an SNN ‚Äî Visual branch outputs a visual feature vector (V-FV); audio branch outputs NOSC codes. Fusion occurs in the integration layer and is regulated by inhibition.\n\nSTC-like plasticity ‚Äî NIMI-STC implements tag‚ÄìPRP (plasticity-related proteins) dynamics to support both rapid acquisition and long-term consolidation.\n\nLightweight readout (LOC-ANN) ‚Äî Classifies spiking firing patterns after fusion and enables fast comparisons of connectivity and plasticity settings.\n\nIncremental benchmarks ‚Äî Evaluated on MNIST/EMNIST/CIFAR-10/100 with class-incremental sequences; AVIM shows lower forgetting and higher final accuracy.\n\n\n\nModel overview\n\nInputs\n\nVisual: CNN ‚Üí Visual Feature Vector (V-FV) ‚Üí SNN L1 (visual layer)\n\nAudio: NOSC generator ‚Üí Audio code ‚Üí SNN L2 (audio layer)\n\n\nIntegration\n\nMultilayer SNN where L3 integrates L1/L2, and L4 provides inhibition to shape population dynamics.\n\n\nPlasticity (NIMI-STC)\n\nSynapses receive a transient tag when activity crosses thresholds; consolidated changes occur by capturing PRPs, yielding stable long-term memory while allowing new learning.\n\n\nReadout (LOC-ANN)\n\nA small ANN reads population firing patterns to produce class labels; convenient for ablations across AMPAR/NMDAR and excitatory/inhibitory connectivity choices.\n\n\n\n\nContinual learning protocol\n\nClass-incremental: New classes are introduced in batches; prior-class samples are not replayed or minimally replayed.\n\nTraining: Online SNN updates with STC-like plasticity; periodic fine-tuning of the readout.\n\nMetrics: Average accuracy, forgetting (performance drop on past classes), and the stability‚Äìplasticity trade-off.\n\n\n\nResults snapshot\n\nAVIM + STC achieves higher retained accuracy and lower forgetting than (i) SNNs without STC and (ii) single-modality SNNs.\n\nTuning L4 inhibition, L2‚ÜîÔ∏éL3 connectivity sparsity, and STC resource thresholds yields better stability‚Äìplasticity balance.\n\nComplementary NOSC audio and visual V-FV increase class separability in the integrated firing patterns, improving LOC-ANN readout.\n\n\n\nAblations\n\nNo-STC vs STC: Without STC, old-class performance degrades rapidly; with STC, long-term retention is maintained.\n\nSingle-modality vs AVIM: Either visual-only or audio-only underperforms; multimodal fusion consistently helps incremental learning.\n\nInhibition strength: Moderate inhibition suppresses interference and improves separability; excessive inhibition harms plasticity.\n\n\n\nTake-aways\n\nIntegrating multisensory fusion with STC-like plasticity in SNNs is a viable route to human-like continual learning.\n\nA compact LOC-ANN readout provides a simple, reproducible evaluation interface.\n\nThe framework is ready to extend to richer, continuous audiovisual streams for fully online learning.\n\n\n\n\n\n\nNote: Place the poster image at images/avim_poster.jpg. Replace the ‚Äúposter (PDF)‚Äù link above with your file path if you plan to host the PDF."
  },
  {
    "objectID": "projects/posts/minimodel.html",
    "href": "projects/posts/minimodel.html",
    "title": "Towards a simplified model of primary visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks (ANNs) have been shown to predict neural responses in primary visual cortex (V1) better than classical models. However, this performance comes at the expense of simplicity because the ANN models typically have many hidden layers with many feature maps in each layer. Here we show that ANN models of V1 can be substantially simplified while retaining high predictive power. To demonstrate this, we first recorded a new dataset of over 29,000 neurons responding to up to 65,000 natural image presentations in mouse V1. We found that ANN models required only two convolutional layers for good performance, with a relatively small first layer. We further found that we could make the second layer small without loss of performance, by fitting a separate ‚Äúminimodel‚Äù to each neuron. Similar simplifications applied for models of monkey V1 neurons. We show that these relatively simple models can nonetheless be useful for tasks such as object and visual texture recognition and we use the models to gain insight into how texture invariance arises in biological neurons.\n\n\npaper | original tweeprint\n\n\nThread by Fengtong Du:\n\nPredicting neural activity is notoriously difficult and requires complicated models. Here we develop simple ‚Äúminimodels‚Äù which explain 70% of neural variance in V1! üê≠üêí\n\n\n\n\n\n\n\n\nWe started with population-level models, fitting all neurons together with 4 shared conv layers. These models performed better than past models because we showed many more images. The model predicted monkey V1 responses well too.\n\n\n\nBut we didn‚Äôt need such a deep network: two convolutional layers were sufficient, in both mice and monkeys. Also, the first layer could be very small, 16 filters, while the second layer did need to be large, in line with the high dimensionality of V1.\n\n\n\n\nThis structure ‚Äì small first convolutional layer and large second convolutional layer ‚Äì was advantageous for performing visual tasks, such as texture classification and image recognition.\n\n\n\n\nNext, can we simplify the wide second layer further? We found that using more neurons to fit the model did NOT help! This suggested that we could fit smaller models to individual neurons.\n\n\n\n\nSo we built a minimodel for each neuron, matching the performance of the best models. On average, mouse minimodels had 32 conv2 filters and monkey minimodels had 7, much fewer than the 320 filters in our previous model.\n\n\n\n\nNow equipped with a minimodel for each neuron, we used them to understand how the visual invariance of a single neuron develops across the model stages. We designed a metric, fraction of category variance (FECV) to measure this invariance.\n\n\n\n\nWe found that instead of gradually increasing, the invariance primarily emerges at the readout stage and is influenced by both pooling size and input channel similarity.\n\n\n\n\nWith these minimodels, we can also visualize the high and low FECV neurons in mouse and monkey V1.\n\n\n\n\nIn summary, we found single-neuron minimodels are just as powerful as larger ones! It offers an accurate and interpretable approach to studying visual computation across different species and experimental contexts. üê≠üêí\n\nHuge thanks to Janelia! Thanks to the GENIE project, the Vivarium staff, Sarah Lindo and Sal DiLisio for surgery, Jon Arnold for designing headbars and coverslips, Dan Flickinger for microscopy, and Jon Arnold and Tobias Goulet for engineering support."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Fengtong Du, Miguel Angel N√∫√±ez-Ochoa, Marius Pachitariu‚Ä†, Carsen Stringer‚Ä†. A simplified minimodel of visual cortical neurons. Nature Communications, 2025.\nCarsen Stringer, Lin Zhong, Atika Syeda, Fengtong Du, Maria Kesa, Marius Pachitariu. Rastermap: A Discovery Method for Neural Population Recordings.¬†bioRxiv, 2023.\nFengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer, Invariant texture recognition in mouse visual cortex, COSYNE, 2021.\nLihong Cao, Fengtong Du, Wenjie Chen, A neural assembly learning method for concept-like cells formation and continual learning, Granted Invention Patent, National Intellectual Property Administration of China (CNIPA), Patent No.¬†ZL202110427115.9, 2021.\nWenjie Chen, Fengtong Du, Ye Wang and Lihong Cao. A Biologically Plausible Audio-Visual Integration Model for Continual Learning, IJCNN, 2021.\nFengtong Du, Yanzhuo Zhou, Wenjie Chen, Lei Yang. Bullet hole detection using series Faster-RCNN and video analysis, ICMV, 2019.\nWenjie Chen, Fengtong Du, Ye Wang, Lihong Cao, Predropout&Inhibition: A Brain-Inspired Method for Convolutional Neural Network, CISP, 2018\nXinyi Zhou, Wei Gong, WenLong Fu, Fengtong Du, Application of deep learning in object detection, ICIS, 2017."
  },
  {
    "objectID": "projects/posts/bullet.html",
    "href": "projects/posts/bullet.html",
    "title": "Bullet hole detection",
    "section": "",
    "text": "Abstract\n\n\n\nDetecting small objects is challenging because of its low resolution and noisy representation. This paper focus on localize the bullet holes on a 4m*4m target surface and determine the shot time and position of new bullet holes on the target surface based on surveillance videos of the target. Under such a condition, bullet holes are extremely small compared with the target surface. In this paper, an improved model based on Faster-RCNN is proposed to solve the problem using two networks in series. The first network is trained using original video frames and obtain coarse locations of bullet holes, the second network is trained using the candidate locations obtained by the first network to get accurate locations. Experiment result shows that the series Faster-RCNN algorithm improves the average precision by 20.3% over the original Faster-RCNN algorithm on our bullet-hole dataset. To determine the shot time and improve detection accuracy, several algorithms have also been proposed, using these algorithms, detection accuracy of shot times and new shot points reaches the same level as human."
  },
  {
    "objectID": "journals/posts/post1.html",
    "href": "journals/posts/post1.html",
    "title": "My First Post",
    "section": "",
    "text": "These are my thoughts on a particular topic."
  },
  {
    "objectID": "journals/post1.html",
    "href": "journals/post1.html",
    "title": "My First Post",
    "section": "",
    "text": "My First Post\nThese are my thoughts on a particular topic."
  }
]