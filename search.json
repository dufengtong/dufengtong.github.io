[
  {
    "objectID": "academia/index.html",
    "href": "academia/index.html",
    "title": "Academic Activities",
    "section": "",
    "text": "2023 - The 10th Statistical Analysis of Neuronal Data (SAND10) Workshop - Talk\n- Towards a simplified model of primary visual cortex\n2021 - Computational and Systems Neuroscience (COSYNE) - Poster\n- Invariant texture recognition in mouse visual cortex, Fengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer"
  },
  {
    "objectID": "academia/index.html#posters-and-talks",
    "href": "academia/index.html#posters-and-talks",
    "title": "Academic Activities",
    "section": "",
    "text": "2023 - The 10th Statistical Analysis of Neuronal Data (SAND10) Workshop - Talk\n- Towards a simplified model of primary visual cortex\n2021 - Computational and Systems Neuroscience (COSYNE) - Poster\n- Invariant texture recognition in mouse visual cortex, Fengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer"
  },
  {
    "objectID": "academia/index.html#teaching-activities",
    "href": "academia/index.html#teaching-activities",
    "title": "Academic Activities",
    "section": "Teaching Activities",
    "text": "Teaching Activities\n2025 ‚Äî Cajal NeuroAI Summer School ‚Äî Teaching Assistant\n- Designed and delivered tutorials on computational neuroscience and deep learning.\n- Mentored students on projects involving foundation models and unsupervised learning using JEPA.\n2023 ‚Äî Probabilistic Machine Learning ‚Äî Teaching Assistant\n- Organized and assisted in teaching the course for neuroscience students at Janelia.\n- Based on the Probabilistic Machine Learning book.\n2021 ‚Äî Neuromatch Academy ‚Äî Teaching Assistant\n- course TA of Neuromatch computational neuroscience course.\n2020 ‚Äî Neuromatch Academy ‚Äî Teaching Assistant\n- course TA of Neuromatch computational neuroscience course."
  },
  {
    "objectID": "journals/index.html",
    "href": "journals/index.html",
    "title": "Journal",
    "section": "",
    "text": "My First Post\n\n\n\n\n\n\n\n\nAug 17, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Du Fengtong (Farah)",
    "section": "",
    "text": "Du Fengtong\n\n\n GitHub\n Twitter\n Google Scholar\n\n\nHi! My name is Fengtong (Farah) Du, a PhD student in the Janelia-JHU Joint Neuroscience Graduate Program, advised by Carsen Stringer.\nMy work bridges computational modeling and neuroscience, with a focus on understanding how visual information is represented in the brain, and how the brain learn from the outside world in an unsupervised way.\nOutside the lab, I enjoy Chinese philosophy, Russian literature, writing and music. I also love occasional spontaneous adventure close to home.\nWelcome to check out my CV!"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Towards a simplified model of primary visual cortex\n\n\nSimplified and interpretable ‚Äúminimodels‚Äù are sufficient to explain complex visual responses in mouse and monkey V1.\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAudio‚ÄìVisual Integration Model (AVIM) for Continual Learning\n\n\nAVIM is a multimodal spiking model that fuses visual and audio inputs and learns with a Synaptic Tagging & Capture‚Äìlike rule. It targets brain-inspired continual learning with reduced catastrophic forgetting.\n\n\n\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nGastroenterologist-level detection of gastric precursor lesion\n\n\nDigestive Endoscopology Recognition.\n\n\n\n\n\nJul 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSports Object Recognition\n\n\nObject detection from basketball and football videos.\n\n\n\n\n\nAug 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBullet hole detection\n\n\nBullet hole detection using series Faster-RCNN and video analysis.\n\n\n\n\n\nJul 1, 2017\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/posts/AVIM.html",
    "href": "projects/posts/AVIM.html",
    "title": "Audio‚ÄìVisual Integration Model (AVIM) for Continual Learning",
    "section": "",
    "text": "Abstract\n\n\n\nWe introduce an Audio‚ÄìVisual Integration Model (AVIM) implemented in a spiking neural network. Visual features (from a compact CNN) and audio codes (Randomized Near-Orthogonal Sparse Codes, NOSC) are integrated in a multi-layer SNN. Learning uses a calcium-based Synaptic Tagging & Capture (STC) mechanism that consolidates useful changes while acquiring new classes, supporting continual learning without heavy rehearsal. We evaluate on MNIST/EMNIST/CIFAR-10/100 under class-incremental protocols and analyze stability‚Äìplasticity behavior and representational dynamics.\n\n\npaper\n\n\nWhat‚Äôs new\n\nBiophysically grounded SNN ‚Äî AVIM is built from multi-compartment Hodgkin‚ÄìHuxley neurons, aligning the computation with cortical biophysics. :contentReferenceoaicite:1\n\nCalcium-based STC learning ‚Äî We implement Synaptic Tagging & Capture as the core plasticity rule, linking tags and PRPs to support consolidation while learning new classes. :contentReferenceoaicite:2\n\nBrain-inspired continual-learning paradigm ‚Äî The paper formalizes a paradigm meant to approximate human lifelong learning and uses it to evaluate models. :contentReferenceoaicite:3\n\nSOTA vs.¬†CL baselines ‚Äî AVIM outperforms OWM, iCaRL, and GEM, and forms stable representations over time. :contentReferenceoaicite:4\n\n\n\nModel at a glance\n\n\n\n\n\n\nInputs\n\nVisual: a compact CNN produces a visual feature vector (V-FV).\n\nAudio: a Randomized Near-Orthogonal Sparse Code (NOSC) encodes the audio stream.\n\n\nSpiking core\n\nMulti-compartment HH neurons with calcium-based STC implement tagging and protein-capture dynamics for consolidation. :contentReferenceoaicite:5\n\n\nIntegration & readout\n\nLayers L1/L2 represent visual/audio streams; L3 integrates; L4 provides inhibition.\n\nA lightweight LOC-ANN reads out class labels from firing patterns (as in the poster schematic).\n\n\n\n\nContinual-learning setup\n\n\n\n\n\n\nScenario: class-incremental sequences without (or with minimal) rehearsal, matching the proposed paradigm. :contentReferenceoaicite:6\n\nBenchmarks: MNIST, EMNIST, CIFAR-10/100.\n\nMetrics: average accuracy, forgetting, stability‚Äìplasticity balance.\n\n\n\nResults (paper highlights)\n\n\n\n\n\n\nSOTA performance & stable representations under the new paradigm, beating OWM/iCaRL/GEM on representative tasks. :contentReferenceoaicite:7\n\nQualitative stability: learned object representations remain stable as new classes are acquired. :contentReferenceoaicite:8\n\n\n\nReference\nChen, W., Du, F., Wang, Y., & Cao, L. A Biologically Plausible Audio-Visual Integration Model for Continual Learning. arXiv:2007.08855 (IJCNN 2021)."
  },
  {
    "objectID": "projects/posts/minimodel.html",
    "href": "projects/posts/minimodel.html",
    "title": "Towards a simplified model of primary visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks (ANNs) have been shown to predict neural responses in primary visual cortex (V1) better than classical models. However, this performance comes at the expense of simplicity because the ANN models typically have many hidden layers with many feature maps in each layer. Here we show that ANN models of V1 can be substantially simplified while retaining high predictive power. To demonstrate this, we first recorded a new dataset of over 29,000 neurons responding to up to 65,000 natural image presentations in mouse V1. We found that ANN models required only two convolutional layers for good performance, with a relatively small first layer. We further found that we could make the second layer small without loss of performance, by fitting a separate ‚Äúminimodel‚Äù to each neuron. Similar simplifications applied for models of monkey V1 neurons. We show that these relatively simple models can nonetheless be useful for tasks such as object and visual texture recognition and we use the models to gain insight into how texture invariance arises in biological neurons.\n\n\npaper | original tweeprint\n\n\nThread by Fengtong Du:\n\nPredicting neural activity is notoriously difficult and requires complicated models. Here we develop simple ‚Äúminimodels‚Äù which explain 70% of neural variance in V1! üê≠üêí\n\n\n\n\n\n\n\n\nWe started with population-level models, fitting all neurons together with 4 shared conv layers. These models performed better than past models because we showed many more images. The model predicted monkey V1 responses well too.\n\n\n\nBut we didn‚Äôt need such a deep network: two convolutional layers were sufficient, in both mice and monkeys. Also, the first layer could be very small, 16 filters, while the second layer did need to be large, in line with the high dimensionality of V1.\n\n\n\n\nThis structure ‚Äì small first convolutional layer and large second convolutional layer ‚Äì was advantageous for performing visual tasks, such as texture classification and image recognition.\n\n\n\n\nNext, can we simplify the wide second layer further? We found that using more neurons to fit the model did NOT help! This suggested that we could fit smaller models to individual neurons.\n\n\n\n\nSo we built a minimodel for each neuron, matching the performance of the best models. On average, mouse minimodels had 32 conv2 filters and monkey minimodels had 7, much fewer than the 320 filters in our previous model.\n\n\n\n\nNow equipped with a minimodel for each neuron, we used them to understand how the visual invariance of a single neuron develops across the model stages. We designed a metric, fraction of category variance (FECV) to measure this invariance.\n\n\n\n\nWe found that instead of gradually increasing, the invariance primarily emerges at the readout stage and is influenced by both pooling size and input channel similarity.\n\n\n\n\nWith these minimodels, we can also visualize the high and low FECV neurons in mouse and monkey V1.\n\n\n\n\nIn summary, we found single-neuron minimodels are just as powerful as larger ones! It offers an accurate and interpretable approach to studying visual computation across different species and experimental contexts. üê≠üêí\n\nHuge thanks to Janelia! Thanks to the GENIE project, the Vivarium staff, Sarah Lindo and Sal DiLisio for surgery, Jon Arnold for designing headbars and coverslips, Dan Flickinger for microscopy, and Jon Arnold and Tobias Goulet for engineering support."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Fengtong Du, Miguel Angel N√∫√±ez-Ochoa, Marius Pachitariu‚Ä†, Carsen Stringer‚Ä†. A simplified minimodel of visual cortical neurons. Nature Communications, 2025.\nCarsen Stringer, Lin Zhong, Atika Syeda, Fengtong Du, Maria Kesa, Marius Pachitariu. Rastermap: A Discovery Method for Neural Population Recordings.¬†bioRxiv, 2023.\nFengtong Du, Sonia Joseph, Marius Pachitariu, Carsen Stringer, Invariant texture recognition in mouse visual cortex, COSYNE, 2021.\nLihong Cao, Fengtong Du, Wenjie Chen, A neural assembly learning method for concept-like cells formation and continual learning, Granted Invention Patent, National Intellectual Property Administration of China (CNIPA), Patent No.¬†ZL202110427115.9, 2021.\nWenjie Chen, Fengtong Du, Ye Wang and Lihong Cao. A Biologically Plausible Audio-Visual Integration Model for Continual Learning, IJCNN, 2021.\nFengtong Du, Yanzhuo Zhou, Wenjie Chen, Lei Yang. Bullet hole detection using series Faster-RCNN and video analysis, ICMV, 2019.\nWenjie Chen, Fengtong Du, Ye Wang, Lihong Cao, Predropout&Inhibition: A Brain-Inspired Method for Convolutional Neural Network, CISP, 2018\nXinyi Zhou, Wei Gong, WenLong Fu, Fengtong Du, Application of deep learning in object detection, ICIS, 2017."
  },
  {
    "objectID": "projects/posts/bullet.html",
    "href": "projects/posts/bullet.html",
    "title": "Bullet hole detection",
    "section": "",
    "text": "Abstract\n\n\n\nDetecting small objects is challenging because of its low resolution and noisy representation. This paper focus on localize the bullet holes on a 4m*4m target surface and determine the shot time and position of new bullet holes on the target surface based on surveillance videos of the target. Under such a condition, bullet holes are extremely small compared with the target surface. In this paper, an improved model based on Faster-RCNN is proposed to solve the problem using two networks in series. The first network is trained using original video frames and obtain coarse locations of bullet holes, the second network is trained using the candidate locations obtained by the first network to get accurate locations. Experiment result shows that the series Faster-RCNN algorithm improves the average precision by 20.3% over the original Faster-RCNN algorithm on our bullet-hole dataset. To determine the shot time and improve detection accuracy, several algorithms have also been proposed, using these algorithms, detection accuracy of shot times and new shot points reaches the same level as human."
  },
  {
    "objectID": "journals/posts/post1.html",
    "href": "journals/posts/post1.html",
    "title": "My First Post",
    "section": "",
    "text": "These are my thoughts on a particular topic."
  },
  {
    "objectID": "journals/post1.html",
    "href": "journals/post1.html",
    "title": "My First Post",
    "section": "",
    "text": "My First Post\nThese are my thoughts on a particular topic."
  }
]